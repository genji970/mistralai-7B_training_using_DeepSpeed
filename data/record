1) 부정적,혐오 문장 제거
2) 문장 clip model을 사용해 reward modeling.
2-1) sentence transformers package이용해 질문과 대답의 관련성을 따져 reward score 추가
2-2) 답에서 가장 질문과 유사도 높은 문장을 찾은 후(sentence tranformers 사용) 그 문장 앞뒤로 전체 문장 갯수 대비 80%에 해당하는 문장들만 추출해서 데이터셋으로 저장.

문제점 1) : 사람이 보기에는 context가 있어서 오히려 정답인데 chatgpt나 sentence transformers의 경량화된 문장 유사도 추출기에서는 reward 점수가 낮게 산출된다.
-> voting이 가장 높은 상태니 관련은 있다. 그러면 reward의 역이 오히려 더 상관성이 높다고 가정. reward의 역이 낮으면 추상적으로, 좀더 고등차원에서 맞는말

최적화 기법들
1) 단순 유사도 외에도 질문 중심 시멘틱 관련성에 집중한 모델을 쓰는 것이 효과적
2) 질문과 답의 유사도 + 질문과 답 중 가장 cosine simil이 높은 문장의 interpolation
2-1) 전체적인 답은 직관적으로 사람이 이해할 수 있는 고차원의 답이지만 기계가 보기에는 직접적인 답이 아닐 수 있다. 그러므로 적절한 답일지라도
cos simil이 낮을 수 있는데 문장의 경우, 전체적인 답에 비해 적절한 내용이 직접적 답일 가능성이 크다.
2-2) 전체 답 값이 높으면 완전히 직접적, 개별 문장의 값이 높아도 직접적. 개별 문장이 맞기만 해도 정답으로 인정하되 전체 답 값이 높으면 완전히 답에 가깝다 가정하면,
전체 답의 simil값에 가중치를 좀 더 높인다.
3) llm (x) , pair 늘려 contrastive learning (x)
4)