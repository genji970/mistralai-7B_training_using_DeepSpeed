from transformers import AutoTokenizer
# Hugging Face datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì„í¬íŠ¸
from datasets import Dataset
import torch # PyTorch í…ì„œ ì—°ì‚°ì„ ìœ„í•´ ì„í¬íŠ¸

# Mistral-7B-v0.3 í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.3")

# íŒ¨ë”© í† í°ì´ ì—†ëŠ” ê²½ìš° [PAD] í† í° ì¶”ê°€
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})

def tokenize(examples, MAX_LEN=520):
    """
    ì£¼ì–´ì§„ ì˜ˆì‹œ ë°ì´í„°ë¥¼ í† í¬ë‚˜ì´ì§•í•˜ê³ , ëª¨ë¸ ì…ë ¥ ë° ë ˆì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤.
    'prompt'/'completion' ë˜ëŠ” 'instruction'/'input'/'output' êµ¬ì¡°ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
    íŒ¨ë”©ì„ ì ìš©í•˜ê³  ë ˆì´ë¸”ì˜ íŒ¨ë”© í† í°ì„ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹í•©ë‹ˆë‹¤.
    ë ˆì´ë¸” ë§ˆìŠ¤í‚¹ì€ PyTorch í…ì„œ ì—°ì‚°ì„ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    Args:
        examples (dict): í† í¬ë‚˜ì´ì§•í•  ë°ì´í„° ì˜ˆì‹œ ë”•ì…”ë„ˆë¦¬.
                         'prompt'ì™€ 'completion' í‚¤ ë˜ëŠ”
                         'instruction', 'input', 'output' í‚¤ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
                         (map í•¨ìˆ˜ì—ì„œ batched=Trueë¡œ ì‚¬ìš© ì‹œ ê° í‚¤ì˜ ê°’ì€ ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•¨)
        MAX_LEN (int): ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´. ì´ ê¸¸ì´ì— ë§ì¶° íŒ¨ë”© ë° ì˜ë¼ë‚´ê¸°ê°€ ìˆ˜í–‰ë©ë‹ˆë‹¤.

    Returns:
        dict: 'input_ids', 'attention_mask', 'labels'ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬.
              ê° ê°’ì€ PyTorch í…ì„œ í˜•íƒœì…ë‹ˆë‹¤.
    """
    EOS_TOKEN = tokenizer.eos_token # End-of-sequence í† í°

    prompts = []
    completions = []

    # ë°ì´í„° êµ¬ì¡°ì— ë”°ë¼ promptì™€ completionì„ ì¤€ë¹„
    # case 1: prompt/completion êµ¬ì¡°
    if "prompt" in examples and "completion" in examples:
        # ì…ë ¥ì´ ë‹¨ì¼ ë¬¸ìì—´ì¼ ìˆ˜ë„ ìˆê³ , map í•¨ìˆ˜ì—ì„œ batched=True ì‹œ ë¦¬ìŠ¤íŠ¸ì¼ ìˆ˜ë„ ìˆìŒ
        _prompts = examples["prompt"] if isinstance(examples["prompt"], list) else [examples["prompt"]]
        _completions = examples["completion"] if isinstance(examples["completion"], list) else [examples["completion"]]
        
        # completion ëì— EOS í† í° ì¶”ê°€
        completions = [c + EOS_TOKEN for c in _completions]
        prompts = _prompts

    # case 2: instruction/input/output êµ¬ì¡° (Alpaca ìŠ¤íƒ€ì¼)
    elif "instruction" in examples and "output" in examples:
        _instructions = examples["instruction"] if isinstance(examples["instruction"], list) else [examples["instruction"]]
        # 'input' í‚¤ê°€ ì—†ì„ ê²½ìš° ë¹ˆ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
        _inputs = examples["input"] if "input" in examples else ["" for _ in _instructions]
        _outputs = examples["output"] if isinstance(examples["output"], list) else [examples["output"]]

        for inst, inp in zip(_instructions, _inputs):
            prompt = f"### Instruction:\n{inst}\n\n"
            if inp and inp.strip(): # inputì´ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì¶”ê°€
                prompt += f"### Input:\n{inp}\n\n"
            prompt += "### Response:\n" # ì‘ë‹µ ì‹œì‘ ë¶€ë¶„
            prompts.append(prompt)
        
        # output ëì— EOS í† í° ì¶”ê°€
        completions = [o + EOS_TOKEN for o in _outputs]

    else:
        # ì§€ì›í•˜ì§€ ì•ŠëŠ” í‚¤ êµ¬ì¡°ì¼ ê²½ìš° ì—ëŸ¬ ë°œìƒ
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í‚¤ êµ¬ì¡°: {examples.keys()}")

    # 1. í”„ë¡¬í”„íŠ¸ í† í¬ë‚˜ì´ì§• (ëª¨ë¸ ì…ë ¥)
    # return_tensors='pt'ë¥¼ ì‚¬ìš©í•˜ì—¬ PyTorch í…ì„œë¡œ ë°˜í™˜
    model_inputs = tokenizer(
        prompts,
        truncation=True,        # MAX_LENë³´ë‹¤ ê¸¸ë©´ ì˜ë¼ëƒ„
        max_length=MAX_LEN,     # ìµœëŒ€ ê¸¸ì´ ì„¤ì •
        padding='max_length',   # ëª¨ë“  ì‹œí€€ìŠ¤ë¥¼ MAX_LENìœ¼ë¡œ íŒ¨ë”©
        return_tensors='pt'     # PyTorch í…ì„œë¡œ ë°˜í™˜
    )

    # 2. ì»´í”Œë¦¬ì…˜ í† í¬ë‚˜ì´ì§• (ë ˆì´ë¸” ìƒì„±ìš©)
    # return_tensors='pt'ë¥¼ ì‚¬ìš©í•˜ì—¬ PyTorch í…ì„œë¡œ ë°˜í™˜
    label_outputs = tokenizer(
        completions,
        truncation=True,
        max_length=MAX_LEN,
        padding='max_length',
        return_tensors='pt'
    )

    # ğŸ”¥ labelsì—ì„œ pad_token_idë¥¼ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
    # PyTorch í…ì„œ ì—°ì‚°ì„ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ ë§ˆìŠ¤í‚¹ ìˆ˜í–‰
    labels = label_outputs["input_ids"].clone() # ì›ë³¸ í…ì„œë¥¼ ë³µì‚¬í•˜ì—¬ ìˆ˜ì •
    labels[labels == tokenizer.pad_token_id] = -100 # íŒ¨ë”© í† í° IDë¥¼ -100ìœ¼ë¡œ ë³€ê²½

    # íŒ¨ë”© ë° ë§ˆìŠ¤í‚¹ í›„ input_idsì™€ labelsì˜ ê¸¸ì´ê°€ ë™ì¼í•œì§€ í™•ì¸ (ë””ë²„ê¹… ëª©ì )
    # map í•¨ìˆ˜ì—ì„œ batched=Trueë¥¼ ì‚¬ìš©í•  ë•Œ ëª¨ë“  ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ê°€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.
    # ì´ì œ input_idsì™€ labels ëª¨ë‘ í…ì„œì´ë¯€ë¡œ .shape[1]ìœ¼ë¡œ ê¸¸ì´ í™•ì¸
    for i in range(model_inputs["input_ids"].shape[0]): # ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë°˜ë³µ
        assert model_inputs["input_ids"].shape[1] == MAX_LEN, \
            f"Input IDs length mismatch at index {i}: Expected {MAX_LEN}, Got {model_inputs['input_ids'].shape[1]}"
        assert labels.shape[1] == MAX_LEN, \
            f"Labels length mismatch at index {i}: Expected {MAX_LEN}, Got {labels.shape[1]}"
        assert model_inputs["input_ids"].shape[1] == labels.shape[1], \
            f"Input IDs and Labels length mismatch at index {i}: Input IDs {model_inputs['input_ids'].shape[1]}, Labels {labels.shape[1]}"

    # ìµœì¢… ë ˆì´ë¸”ì„ model_inputs ë”•ì…”ë„ˆë¦¬ì— ì¶”ê°€
    model_inputs["labels"] = labels
    return model_inputs