def inspect_tokenized_dataset(dataset, num_samples=3):
    print(f"\nğŸ” Inspecting first {num_samples} samples in tokenized dataset...")

    for i in range(min(num_samples, len(dataset))):
        sample = dataset[i]
        print(f"\n=== Sample {i} ===")
        print("input_ids:", sample.get("input_ids"))
        print("attention_mask:", sample.get("attention_mask"))
        print("labels:", sample.get("labels"))

        # íƒ€ì… í™•ì¸
        if not isinstance(sample.get("labels"), list):
            print(f"âŒ Sample {i}: labelsê°€ listê°€ ì•„ë‹˜ â†’ {type(sample.get('labels'))}")
        elif not all(isinstance(x, int) for x in sample["labels"]):
            print(f"âŒ Sample {i}: labels ë‚´ë¶€ì— intê°€ ì•„ë‹Œ ê°’ ì¡´ì¬")
        else:
            print(f"âœ… Sample {i}: labels êµ¬ì¡° ì •ìƒ")

    # ê¸¸ì´ ë¹„êµ
    input_lens = [len(s["input_ids"]) for s in dataset[:num_samples]]
    label_lens = [len(s["labels"]) for s in dataset[:num_samples]]

    print("\nğŸ“ input_ids ê¸¸ì´:", input_lens)
    print("ğŸ“ labels ê¸¸ì´:   ", label_lens)
